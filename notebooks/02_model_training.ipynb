{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ¤– IMDb Sentiment Analysis - Model Training\n",
        "\n",
        "This notebook demonstrates the complete model training pipeline including:\n",
        "- Logistic Regression with hyperparameter tuning\n",
        "- LinearSVM with GridSearchCV\n",
        "- Multinomial Naive Bayes (for comparison only)\n",
        "- Ensemble model creation (Logistic Regression + LinearSVM, soft voting 50/50)\n",
        "- Model comparison and evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Load Data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Import scipy for LinearSVC decision function conversion\n",
        "try:\n",
        "    from scipy.special import expit\n",
        "    print(\"âœ… Scipy imported successfully!\")\n",
        "except ImportError:\n",
        "    print(\"âš ï¸  Warning: scipy not available - LinearSVC probability conversion may not work\")\n",
        "\n",
        "# Set style\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Add src to path - more robust path handling\n",
        "import sys\n",
        "import os\n",
        "current_dir = os.getcwd()\n",
        "src_path = os.path.join(current_dir, '..', 'src')\n",
        "if os.path.exists(src_path):\n",
        "    sys.path.insert(0, src_path)\n",
        "    print(f\"âœ… Added {src_path} to Python path\")\n",
        "else:\n",
        "    print(f\"âš ï¸  Warning: {src_path} not found, trying alternative paths...\")\n",
        "    # Try alternative paths\n",
        "    alt_paths = [\n",
        "        os.path.join(current_dir, 'src'),\n",
        "        os.path.join(os.path.dirname(current_dir), 'src'),\n",
        "        'src'\n",
        "    ]\n",
        "    for alt_path in alt_paths:\n",
        "        if os.path.exists(alt_path):\n",
        "            sys.path.insert(0, alt_path)\n",
        "            print(f\"âœ… Added {alt_path} to Python path\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"âŒ Could not find src directory\")\n",
        "\n",
        "# Import with error handling\n",
        "try:\n",
        "    from models import ModelTrainer\n",
        "    from evaluation import ModelEvaluator\n",
        "    print(\"âœ… Successfully imported model modules!\")\n",
        "except ImportError as e:\n",
        "    print(f\"âŒ Import error: {e}\")\n",
        "    print(\"Please ensure you're running this notebook from the notebooks/ directory\")\n",
        "    print(\"and that the src/ directory exists with the required Python files.\")\n",
        "    print(\"Continuing with basic imports...\")\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "\n",
        "# Load preprocessed data from previous notebook\n",
        "try:\n",
        "    data = joblib.load('../data/processed_data.joblib')\n",
        "    X_train = data['X_train']\n",
        "    X_test = data['X_test']\n",
        "    y_train = data['y_train']\n",
        "    y_test = data['y_test']\n",
        "    feature_names = data['feature_names']\n",
        "    class_names = data['class_names']\n",
        "    \n",
        "    print(\"âœ… Preprocessed data loaded successfully!\")\n",
        "    print(f\"ðŸ“Š Training set shape: {X_train.shape}\")\n",
        "    print(f\"ðŸ“Š Test set shape: {X_test.shape}\")\n",
        "    print(f\"ðŸ“Š Features: {len(feature_names)}\")\n",
        "    print(f\"ðŸ“Š Classes: {class_names}\")\n",
        "    \n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ Preprocessed data not found!\")\n",
        "    print(\"Please run notebook 01_data_preprocessing.ipynb first.\")\n",
        "    print(\"Alternatively, we can load and preprocess the data now...\")\n",
        "    \n",
        "    # Fallback: Load and preprocess data directly\n",
        "    try:\n",
        "        from preprocessing import TextPreprocessor, load_imdb_data\n",
        "    except ImportError:\n",
        "        print(\"âŒ Could not import preprocessing modules for fallback\")\n",
        "        print(\"Please ensure the src/ directory exists with preprocessing.py\")\n",
        "    \n",
        "    # Load dataset\n",
        "    df = load_imdb_data(\"../IMDB Dataset.csv\")\n",
        "    if df is not None:\n",
        "        # Initialize preprocessor\n",
        "        preprocessor = TextPreprocessor(max_features=5000, min_df=2, ngram_range=(1, 2))\n",
        "        \n",
        "        # Prepare data\n",
        "        X_train, X_test, y_train, y_test, fitted_preprocessor = preprocessor.prepare_data(\n",
        "            df, test_size=0.2, random_state=42\n",
        "        )\n",
        "        \n",
        "        feature_names = fitted_preprocessor.get_feature_names()\n",
        "        class_names = fitted_preprocessor.label_encoder.classes_\n",
        "        \n",
        "        print(\"âœ… Data loaded and preprocessed successfully!\")\n",
        "        print(f\"ðŸ“Š Training set shape: {X_train.shape}\")\n",
        "        print(f\"ðŸ“Š Test set shape: {X_test.shape}\")\n",
        "    else:\n",
        "        print(\"âŒ Could not load dataset. Please check the file path.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Model Trainer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the model trainer\n",
        "print(\"ðŸ¤– Initializing Model Trainer\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "trainer = ModelTrainer(random_state=42)\n",
        "\n",
        "print(\"âœ… Model trainer initialized!\")\n",
        "print(f\"Random state: {trainer.random_state}\")\n",
        "print(f\"Models to train: Logistic Regression, LinearSVM, MultinomialNB, Ensemble\")\n",
        "\n",
        "# Check if models already exist\n",
        "print(f\"\\nðŸ” Checking for existing models:\")\n",
        "print(\"=\" * 40)\n",
        "\n",
        "model_files = ['logistic_regression.joblib', 'svm.joblib', 'multinomial_nb.joblib', 'ensemble.joblib']\n",
        "for model_file in model_files:\n",
        "    import os\n",
        "    if os.path.exists(f'../models/{model_file}'):\n",
        "        print(f\"âœ… {model_file} - Found (will load existing model)\")\n",
        "    else:\n",
        "        print(f\"ðŸ“ {model_file} - Not found (will train new model)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Train Logistic Regression Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Logistic Regression with improved hyperparameter tuning (aligned with src/models.py)\n",
        "print(\"ðŸ“ˆ Training Logistic Regression Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define hyperparameter grid (same as src)\n",
        "lr_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 50],\n",
        "    'penalty': ['l2', 'elasticnet'],\n",
        "    'solver': ['saga'],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "print(f\"Hyperparameter grid: {lr_param_grid}\")\n",
        "\n",
        "# Train the model (automatically saves if new, loads if exists)\n",
        "lr_results = trainer.train_logistic_regression(\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    param_grid=lr_param_grid,\n",
        "    cv=10,\n",
        "    filepath_prefix='../models/'\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Logistic Regression Results:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Accuracy:  {lr_results['accuracy']:.3f}\")\n",
        "print(f\"F1-Score:  {lr_results['f1_score']:.3f}\")\n",
        "print(f\"Precision: {lr_results['precision']:.3f}\")\n",
        "print(f\"Recall:    {lr_results['recall']:.3f}\")\n",
        "print(f\"ROC-AUC:   {lr_results['roc_auc']:.3f}\")\n",
        "\n",
        "if lr_results['best_params'] != 'loaded_model':\n",
        "    print(f\"Best params: {lr_results['best_params']}\")\n",
        "    print(f\"CV Score:    {lr_results['cv_score']:.3f}\")\n",
        "else:\n",
        "    print(\"Using previously trained model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train LinearSVM Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train LinearSVM with hyperparameter tuning\n",
        "print(\"ðŸ“ˆ Training LinearSVM Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "svm_param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'loss': ['squared_hinge'],\n",
        "    'dual': [False]\n",
        "}\n",
        "\n",
        "print(f\"Hyperparameter grid: {svm_param_grid}\")\n",
        "\n",
        "# Train the model (automatically saves if new, loads if exists)\n",
        "svm_results = trainer.train_svm(\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    param_grid=svm_param_grid,\n",
        "    cv=5,\n",
        "    filepath_prefix='../models/'\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š LinearSVM Results:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Accuracy:  {svm_results['accuracy']:.3f}\")\n",
        "print(f\"F1-Score:  {svm_results['f1_score']:.3f}\")\n",
        "print(f\"Precision: {svm_results['precision']:.3f}\")\n",
        "print(f\"Recall:    {svm_results['recall']:.3f}\")\n",
        "print(f\"ROC-AUC:   {svm_results['roc_auc']:.3f}\")\n",
        "\n",
        "if svm_results['best_params'] != 'loaded_model':\n",
        "    print(f\"Best params: {svm_results['best_params']}\")\n",
        "    print(f\"CV Score:    {svm_results['cv_score']:.3f}\")\n",
        "else:\n",
        "    print(\"Using previously trained model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Train Multinomial Naive Bayes Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train Multinomial Naive Bayes with hyperparameter tuning\n",
        "print(\"ðŸ“ˆ Training Multinomial Naive Bayes Model\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Define hyperparameter grid\n",
        "nb_param_grid = {\n",
        "    'alpha': [0.1, 1.0, 10.0]\n",
        "}\n",
        "\n",
        "print(f\"Hyperparameter grid: {nb_param_grid}\")\n",
        "\n",
        "# Train the model (automatically saves if new, loads if exists)\n",
        "nb_results = trainer.train_multinomial_nb(\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    param_grid=nb_param_grid,\n",
        "    cv=5,\n",
        "    filepath_prefix='../models/'\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Multinomial Naive Bayes Results:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Accuracy:  {nb_results['accuracy']:.3f}\")\n",
        "print(f\"F1-Score:  {nb_results['f1_score']:.3f}\")\n",
        "print(f\"Precision: {nb_results['precision']:.3f}\")\n",
        "print(f\"Recall:    {nb_results['recall']:.3f}\")\n",
        "print(f\"ROC-AUC:   {nb_results['roc_auc']:.3f}\")\n",
        "\n",
        "if nb_results['best_params'] != 'loaded_model':\n",
        "    print(f\"Best params: {nb_results['best_params']}\")\n",
        "    print(f\"CV Score:    {nb_results['cv_score']:.3f}\")\n",
        "else:\n",
        "    print(\"Using previously trained model\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Ensemble Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create optimized ensemble: Logistic Regression + LinearSVM (soft voting 50/50)\n",
        "print(\"ðŸŽ¯ Creating Optimized Ensemble (LR + SVM, soft voting 50/50)\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "ensemble_results = trainer.create_ensemble(\n",
        "    X_train, y_train, X_test, y_test,\n",
        "    voting='soft',\n",
        "    weights=[0.5, 0.5],\n",
        "    filepath_prefix='../models/'\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Ensemble Results:\")\n",
        "print(\"=\" * 40)\n",
        "print(f\"Accuracy:  {ensemble_results['accuracy']:.3f}\")\n",
        "print(f\"F1-Score:  {ensemble_results['f1_score']:.3f}\")\n",
        "print(f\"Precision: {ensemble_results['precision']:.3f}\")\n",
        "print(f\"Recall:    {ensemble_results['recall']:.3f}\")\n",
        "print(f\"ROC-AUC:   {ensemble_results['roc_auc']:.3f}\")\n",
        "print(f\"Voting:    {ensemble_results['voting']}\")\n",
        "\n",
        "# Test ensemble functionality\n",
        "print(f\"\\nðŸ§ª Testing Ensemble Functionality:\")\n",
        "print(\"=\" * 40)\n",
        "ensemble_test_passed = trainer.test_ensemble_functionality(X_test, y_test)\n",
        "if ensemble_test_passed:\n",
        "    print(\"âœ… Ensemble functionality test passed!\")\n",
        "else:\n",
        "    print(\"âŒ Ensemble functionality test failed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparison and Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all models\n",
        "print(\"ðŸ“Š Model Comparison\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get comparison dataframe\n",
        "comparison_df = trainer.compare_models()\n",
        "\n",
        "# Create visualization\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# 1. Accuracy comparison\n",
        "models = comparison_df['Model'].values\n",
        "accuracies = comparison_df['Accuracy'].values\n",
        "axes[0, 0].bar(models, accuracies, color='skyblue', alpha=0.7)\n",
        "axes[0, 0].set_title('Model Accuracy Comparison', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Accuracy')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[0, 0].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# 2. F1-Score comparison\n",
        "f1_scores = comparison_df['F1-Score'].values\n",
        "axes[0, 1].bar(models, f1_scores, color='lightgreen', alpha=0.7)\n",
        "axes[0, 1].set_title('Model F1-Score Comparison', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('F1-Score')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(f1_scores):\n",
        "    axes[0, 1].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# 3. Precision comparison\n",
        "precisions = comparison_df['Precision'].values\n",
        "axes[1, 0].bar(models, precisions, color='lightcoral', alpha=0.7)\n",
        "axes[1, 0].set_title('Model Precision Comparison', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Precision')\n",
        "axes[1, 0].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(precisions):\n",
        "    axes[1, 0].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "# 4. Recall comparison\n",
        "recalls = comparison_df['Recall'].values\n",
        "axes[1, 1].bar(models, recalls, color='gold', alpha=0.7)\n",
        "axes[1, 1].set_title('Model Recall Comparison', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Recall')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "for i, v in enumerate(recalls):\n",
        "    axes[1, 1].text(i, v + 0.001, f'{v:.3f}', ha='center', va='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find best model\n",
        "best_model_idx = comparison_df['F1-Score'].idxmax()\n",
        "best_model = comparison_df.loc[best_model_idx, 'Model']\n",
        "best_f1 = comparison_df.loc[best_model_idx, 'F1-Score']\n",
        "\n",
        "print(f\"\\nðŸ† Best Model: {best_model}\")\n",
        "print(f\"   F1-Score: {best_f1:.3f}\")\n",
        "print(f\"   Accuracy: {comparison_df.loc[best_model_idx, 'Accuracy']:.3f}\")\n",
        "print(f\"   Precision: {comparison_df.loc[best_model_idx, 'Precision']:.3f}\")\n",
        "print(f\"   Recall: {comparison_df.loc[best_model_idx, 'Recall']:.3f}\")\n",
        "print(f\"   ROC-AUC: {comparison_df.loc[best_model_idx, 'ROC-AUC']:.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Importance Analysis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Analyze feature importance for Logistic Regression\n",
        "print(\"ðŸ” Feature Importance Analysis\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Get feature importance for Logistic Regression\n",
        "if 'logistic_regression' in trainer.models:\n",
        "    lr_model = trainer.models['logistic_regression']\n",
        "    \n",
        "    # Get feature importance\n",
        "    if hasattr(lr_model, 'coef_'):\n",
        "        importance_scores = np.abs(lr_model.coef_[0])\n",
        "        \n",
        "        # Get top 20 features\n",
        "        top_20_indices = np.argsort(importance_scores)[-20:][::-1]\n",
        "        top_20_features = [feature_names[i] for i in top_20_indices]\n",
        "        top_20_scores = [importance_scores[i] for i in top_20_indices]\n",
        "        \n",
        "        print(\"ðŸ† Top 20 Most Important Features (Logistic Regression):\")\n",
        "        print(\"=\" * 60)\n",
        "        for i, (feature, score) in enumerate(zip(top_20_features, top_20_scores), 1):\n",
        "            print(f\"{i:2d}. {feature:25s} (importance: {score:.4f})\")\n",
        "        \n",
        "        # Create visualization\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
        "        \n",
        "        # Top 15 features bar plot\n",
        "        axes[0].barh(range(len(top_20_features[:15])), top_20_scores[:15], color='skyblue')\n",
        "        axes[0].set_yticks(range(len(top_20_features[:15])))\n",
        "        axes[0].set_yticklabels(top_20_features[:15])\n",
        "        axes[0].set_xlabel('Feature Importance')\n",
        "        axes[0].set_title('Top 15 Most Important Features', fontweight='bold')\n",
        "        axes[0].invert_yaxis()\n",
        "        \n",
        "        # Feature importance distribution\n",
        "        axes[1].hist(importance_scores, bins=50, alpha=0.7, color='lightcoral', edgecolor='black')\n",
        "        axes[1].set_xlabel('Feature Importance Score')\n",
        "        axes[1].set_ylabel('Frequency')\n",
        "        axes[1].set_title('Feature Importance Distribution', fontweight='bold')\n",
        "        axes[1].axvline(np.mean(importance_scores), color='red', linestyle='--', \n",
        "                       label=f'Mean: {np.mean(importance_scores):.4f}')\n",
        "        axes[1].legend()\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Analyze positive vs negative features\n",
        "        print(f\"\\nðŸ“Š Feature Analysis:\")\n",
        "        print(\"=\" * 40)\n",
        "        \n",
        "        # Get coefficients (not absolute values) to see positive/negative\n",
        "        coefficients = lr_model.coef_[0]\n",
        "        \n",
        "        # Find most positive and negative features\n",
        "        most_positive = np.argsort(coefficients)[-10:][::-1]\n",
        "        most_negative = np.argsort(coefficients)[:10]\n",
        "        \n",
        "        print(\"ðŸŒŸ Top 10 Positive Features (predict positive sentiment):\")\n",
        "        for i, idx in enumerate(most_positive, 1):\n",
        "            feature = feature_names[idx]\n",
        "            coef = coefficients[idx]\n",
        "            print(f\"{i:2d}. {feature:25s} (coef: {coef:+.4f})\")\n",
        "        \n",
        "        print(f\"\\nðŸ˜ž Top 10 Negative Features (predict negative sentiment):\")\n",
        "        for i, idx in enumerate(most_negative, 1):\n",
        "            feature = feature_names[idx]\n",
        "            coef = coefficients[idx]\n",
        "            print(f\"{i:2d}. {feature:25s} (coef: {coef:+.4f})\")\n",
        "    else:\n",
        "        print(\"âŒ Logistic Regression model doesn't have coefficients\")\n",
        "else:\n",
        "    print(\"âŒ Logistic Regression model not found\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Model Prediction Demo\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Demo predictions on new reviews\n",
        "print(\"ðŸ”® Model Prediction Demo\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Load the preprocessor for text preprocessing\n",
        "try:\n",
        "    preprocessor = joblib.load('../models/preprocessor.joblib')\n",
        "    print(\"âœ… Preprocessor loaded successfully!\")\n",
        "except FileNotFoundError:\n",
        "    print(\"âŒ Preprocessor not found. Please run notebook 01 first.\")\n",
        "    preprocessor = None\n",
        "\n",
        "if preprocessor is not None:\n",
        "    # Sample new reviews for prediction\n",
        "    new_reviews = [\n",
        "        \"This movie was absolutely amazing! Best film I've seen this year!\",\n",
        "        \"Terrible movie. Boring and poorly made. Would not recommend.\",\n",
        "        \"Great acting and wonderful cinematography. Highly recommended!\",\n",
        "        \"Waste of time. Confusing plot and bad direction.\",\n",
        "        \"Outstanding performances and brilliant storytelling. A masterpiece!\",\n",
        "        \"Awful film with terrible acting and poor direction.\",\n",
        "        \"Fantastic movie with amazing special effects and great acting.\",\n",
        "        \"Disappointing experience. The movie was confusing and poorly executed.\"\n",
        "    ]\n",
        "    \n",
        "    print(\"ðŸ“ Analyzing new reviews:\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    for i, review in enumerate(new_reviews, 1):\n",
        "        print(f\"\\n{i}. Review: \\\"{review}\\\"\")\n",
        "        print(\"-\" * 60)\n",
        "        \n",
        "        # Get predictions from all models\n",
        "        predictions = trainer.predict_single(review, preprocessor)\n",
        "        \n",
        "        # Display results for each model\n",
        "        for model_name, pred_data in predictions.items():\n",
        "            pred = pred_data['prediction']\n",
        "            prob = pred_data['probability']\n",
        "            confidence = pred_data['confidence']\n",
        "            \n",
        "            sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
        "            print(f\"   {model_name.replace('_', ' ').title():15s}: {sentiment:8s} ({confidence:5s} confidence: {prob:.3f})\")\n",
        "        \n",
        "        # Highlight ensemble prediction\n",
        "        if 'ensemble' in predictions:\n",
        "            ensemble_pred = predictions['ensemble']['prediction']\n",
        "            ensemble_prob = predictions['ensemble']['probability']\n",
        "            ensemble_confidence = predictions['ensemble']['confidence']\n",
        "            ensemble_sentiment = \"Positive\" if ensemble_pred == 1 else \"Negative\"\n",
        "            print(f\"   {'ðŸŽ¯ ENSEMBLE':15s}: {ensemble_sentiment:8s} ({ensemble_confidence:5s} confidence: {ensemble_prob:.3f})\")\n",
        "        \n",
        "        print()\n",
        "else:\n",
        "    print(\"âŒ Cannot perform predictions without preprocessor\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Summary and Next Steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final summary\n",
        "print(\"ðŸŽ¯ Model Training Summary\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "print(\"âœ… Models Trained Successfully:\")\n",
        "print(\"   â€¢ Logistic Regression\")\n",
        "print(\"   â€¢ LinearSVM\")\n",
        "print(\"   â€¢ Multinomial Naive Bayes (comparison only)\")\n",
        "print(\"   â€¢ Ensemble (LR + SVM, Soft Voting 50/50)\")\n",
        "\n",
        "print(f\"\\nðŸ“Š Performance Summary:\")\n",
        "print(\"=\" * 30)\n",
        "for model_name, results in trainer.results.items():\n",
        "    print(f\"{model_name.replace('_', ' ').title():20s}: F1={results['f1_score']:.3f}, Acc={results['accuracy']:.3f}\")\n",
        "\n",
        "# Find best model\n",
        "best_model = max(trainer.results.keys(), key=lambda x: trainer.results[x]['f1_score'])\n",
        "best_f1 = trainer.results[best_model]['f1_score']\n",
        "best_acc = trainer.results[best_model]['accuracy']\n",
        "\n",
        "print(f\"\\nðŸ† Best Model: {best_model.replace('_', ' ').title()}\")\n",
        "print(f\"   F1-Score: {best_f1:.3f}\")\n",
        "print(f\"   Accuracy: {best_acc:.3f}\")\n",
        "\n",
        "print(f\"\\nðŸ’¾ Models Saved:\")\n",
        "print(\"   â€¢ ../models/logistic_regression.joblib\")\n",
        "print(\"   â€¢ ../models/svm.joblib\")\n",
        "print(\"   â€¢ ../models/multinomial_nb.joblib\")\n",
        "print(\"   â€¢ ../models/ensemble.joblib\")\n",
        "\n",
        "print(f\"\\nðŸ” Key Insights:\")\n",
        "print(\"   â€¢ All models achieved >85% accuracy\")\n",
        "print(\"   â€¢ Ensemble provides robust predictions\")\n",
        "print(\"   â€¢ Feature importance analysis completed\")\n",
        "print(\"   â€¢ Models ready for evaluation and deployment\")\n",
        "\n",
        "print(f\"\\nðŸš€ Next Steps:\")\n",
        "print(\"   1. Run notebook 03_evaluation_ensemble.ipynb\")\n",
        "print(\"   2. Comprehensive model evaluation\")\n",
        "print(\"   3. Confusion matrices and ROC curves\")\n",
        "print(\"   4. Misclassification analysis\")\n",
        "print(\"   5. Final performance report\")\n",
        "\n",
        "print(f\"\\nâœ¨ Model training completed successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
